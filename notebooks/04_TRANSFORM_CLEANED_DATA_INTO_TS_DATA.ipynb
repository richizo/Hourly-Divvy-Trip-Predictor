{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transform the data into a complete time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:44:04.683347093Z",
     "start_time": "2024-01-02T15:44:04.293743556Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.paths import CLEANED_DATA, TEMPORARY_DATA, TRANSFORMED_DATA, GEOGRAPHICAL_DATA\n",
    "from src.data_transformations import add_missing_slots\n",
    "from src.miscellaneous import add_rounded_coordinates_to_dataframe, add_column_of_rounded_points, make_new_station_ids,save_dict, add_column_of_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the cleaned 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:13:28.755460302Z",
     "start_time": "2024-01-02T14:13:28.289973287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>stop_latitude</th>\n",
       "      <th>stop_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-21 20:05:42</td>\n",
       "      <td>2023-01-21 20:16:33</td>\n",
       "      <td>41.924074</td>\n",
       "      <td>-87.646278</td>\n",
       "      <td>41.930000</td>\n",
       "      <td>-87.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-10 15:37:36</td>\n",
       "      <td>2023-01-10 15:46:05</td>\n",
       "      <td>41.799568</td>\n",
       "      <td>-87.594747</td>\n",
       "      <td>41.809835</td>\n",
       "      <td>-87.599383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-02 07:51:57</td>\n",
       "      <td>2023-01-02 08:05:11</td>\n",
       "      <td>42.008571</td>\n",
       "      <td>-87.690483</td>\n",
       "      <td>42.039742</td>\n",
       "      <td>-87.699413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-22 10:52:58</td>\n",
       "      <td>2023-01-22 11:01:44</td>\n",
       "      <td>41.799568</td>\n",
       "      <td>-87.594747</td>\n",
       "      <td>41.809835</td>\n",
       "      <td>-87.599383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-12 13:58:01</td>\n",
       "      <td>2023-01-12 14:13:20</td>\n",
       "      <td>41.799568</td>\n",
       "      <td>-87.594747</td>\n",
       "      <td>41.809835</td>\n",
       "      <td>-87.599383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time           stop_time  start_latitude  start_longitude  \\\n",
       "0 2023-01-21 20:05:42 2023-01-21 20:16:33       41.924074       -87.646278   \n",
       "1 2023-01-10 15:37:36 2023-01-10 15:46:05       41.799568       -87.594747   \n",
       "2 2023-01-02 07:51:57 2023-01-02 08:05:11       42.008571       -87.690483   \n",
       "3 2023-01-22 10:52:58 2023-01-22 11:01:44       41.799568       -87.594747   \n",
       "4 2023-01-12 13:58:01 2023-01-12 14:13:20       41.799568       -87.594747   \n",
       "\n",
       "   stop_latitude  stop_longitude  \n",
       "0      41.930000      -87.640000  \n",
       "1      41.809835      -87.599383  \n",
       "2      42.039742      -87.699413  \n",
       "3      41.809835      -87.599383  \n",
       "4      41.809835      -87.599383  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = pd.read_parquet(path = CLEANED_DATA/\"final.parquet\")\n",
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>stop_latitude</th>\n",
       "      <th>stop_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80659</th>\n",
       "      <td>2023-01-01 00:01:58</td>\n",
       "      <td>2023-01-01 00:02:41</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>-87.590000</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>-87.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189816</th>\n",
       "      <td>2023-01-01 00:02:06</td>\n",
       "      <td>2023-01-01 00:29:46</td>\n",
       "      <td>41.891847</td>\n",
       "      <td>-87.620580</td>\n",
       "      <td>41.890847</td>\n",
       "      <td>-87.618617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185198</th>\n",
       "      <td>2023-01-01 00:03:26</td>\n",
       "      <td>2023-01-01 00:07:23</td>\n",
       "      <td>42.001139</td>\n",
       "      <td>-87.661256</td>\n",
       "      <td>42.001044</td>\n",
       "      <td>-87.661198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106410</th>\n",
       "      <td>2023-01-01 00:04:07</td>\n",
       "      <td>2023-01-01 00:13:56</td>\n",
       "      <td>41.968885</td>\n",
       "      <td>-87.684001</td>\n",
       "      <td>41.973815</td>\n",
       "      <td>-87.659660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169407</th>\n",
       "      <td>2023-01-01 00:04:27</td>\n",
       "      <td>2023-01-01 00:16:52</td>\n",
       "      <td>41.961545</td>\n",
       "      <td>-87.666189</td>\n",
       "      <td>41.961588</td>\n",
       "      <td>-87.666036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104106</th>\n",
       "      <td>2023-01-01 01:26:58</td>\n",
       "      <td>2023-01-01 01:31:58</td>\n",
       "      <td>41.802406</td>\n",
       "      <td>-87.586924</td>\n",
       "      <td>41.799568</td>\n",
       "      <td>-87.594747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72540</th>\n",
       "      <td>2023-01-01 01:27:01</td>\n",
       "      <td>2023-01-01 01:47:51</td>\n",
       "      <td>41.901304</td>\n",
       "      <td>-87.677547</td>\n",
       "      <td>41.920000</td>\n",
       "      <td>-87.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70255</th>\n",
       "      <td>2023-01-01 01:27:05</td>\n",
       "      <td>2023-01-01 01:41:08</td>\n",
       "      <td>41.932261</td>\n",
       "      <td>-87.652751</td>\n",
       "      <td>41.940000</td>\n",
       "      <td>-87.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98794</th>\n",
       "      <td>2023-01-01 01:27:13</td>\n",
       "      <td>2023-01-01 01:33:52</td>\n",
       "      <td>41.925858</td>\n",
       "      <td>-87.638973</td>\n",
       "      <td>41.928712</td>\n",
       "      <td>-87.653833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31799</th>\n",
       "      <td>2023-01-01 01:27:23</td>\n",
       "      <td>2023-01-01 01:34:51</td>\n",
       "      <td>41.920104</td>\n",
       "      <td>-87.648776</td>\n",
       "      <td>41.944540</td>\n",
       "      <td>-87.654678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time           stop_time  start_latitude  \\\n",
       "80659  2023-01-01 00:01:58 2023-01-01 00:02:41       41.800000   \n",
       "189816 2023-01-01 00:02:06 2023-01-01 00:29:46       41.891847   \n",
       "185198 2023-01-01 00:03:26 2023-01-01 00:07:23       42.001139   \n",
       "106410 2023-01-01 00:04:07 2023-01-01 00:13:56       41.968885   \n",
       "169407 2023-01-01 00:04:27 2023-01-01 00:16:52       41.961545   \n",
       "...                    ...                 ...             ...   \n",
       "104106 2023-01-01 01:26:58 2023-01-01 01:31:58       41.802406   \n",
       "72540  2023-01-01 01:27:01 2023-01-01 01:47:51       41.901304   \n",
       "70255  2023-01-01 01:27:05 2023-01-01 01:41:08       41.932261   \n",
       "98794  2023-01-01 01:27:13 2023-01-01 01:33:52       41.925858   \n",
       "31799  2023-01-01 01:27:23 2023-01-01 01:34:51       41.920104   \n",
       "\n",
       "        start_longitude  stop_latitude  stop_longitude  \n",
       "80659        -87.590000      41.800000      -87.590000  \n",
       "189816       -87.620580      41.890847      -87.618617  \n",
       "185198       -87.661256      42.001044      -87.661198  \n",
       "106410       -87.684001      41.973815      -87.659660  \n",
       "169407       -87.666189      41.961588      -87.666036  \n",
       "...                 ...            ...             ...  \n",
       "104106       -87.586924      41.799568      -87.594747  \n",
       "72540        -87.677547      41.920000      -87.650000  \n",
       "70255        -87.652751      41.940000      -87.660000  \n",
       "98794        -87.638973      41.928712      -87.653833  \n",
       "31799        -87.648776      41.944540      -87.654678  \n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.sort_values(by = \"start_time\").head(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the time stamps to hour-based values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:13:29.034557841Z",
     "start_time": "2024-01-02T14:13:28.802676497Z"
    }
   },
   "outputs": [],
   "source": [
    "trips[\"start_hour\"] = trips[\"start_time\"].dt.floor(\"H\")\n",
    "trips[\"stop_hour\"] = trips[\"stop_time\"].dt.floor(\"H\")\n",
    "\n",
    "trips = trips.drop(columns = [\"start_time\", \"stop_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:13:29.118011018Z",
     "start_time": "2024-01-02T14:13:29.029898009Z"
    }
   },
   "outputs": [],
   "source": [
    "starts = trips[\n",
    "    [\n",
    "        \"start_hour\", \"start_latitude\", \"start_longitude\"\n",
    "    ]\n",
    "] \n",
    "\n",
    "\n",
    "stops = trips[\n",
    "    [\n",
    "        \"stop_hour\", \"stop_latitude\", \"stop_longitude\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save original coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:13:31.177227859Z",
     "start_time": "2024-01-02T14:13:29.121545413Z"
    }
   },
   "outputs": [],
   "source": [
    "starts[[\"start_latitude\", \"start_longitude\"]].to_parquet(path = GEOGRAPHICAL_DATA/ \"original_start_coordinates.parquet\") \n",
    "\n",
    "stops[[\"stop_latitude\", \"stop_longitude\"]].to_parquet(path = GEOGRAPHICAL_DATA/ \"original_stop_coordinates.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Round the coordinates to two decimal places to make grouping easier, and remove the old latitude and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:13:35.006191716Z",
     "start_time": "2024-01-02T14:13:31.173490070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5495778/5495778 [00:32<00:00, 167228.90it/s]\n",
      "100%|██████████| 5495778/5495778 [00:30<00:00, 180203.74it/s]\n"
     ]
    }
   ],
   "source": [
    "add_rounded_coordinates_to_dataframe(data = starts, decimal_places = 3, start_or_stop = \"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:34.997997211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5495778/5495778 [00:30<00:00, 177541.59it/s]\n",
      "100%|██████████| 5495778/5495778 [00:30<00:00, 177353.85it/s]\n"
     ]
    }
   ],
   "source": [
    "    add_rounded_coordinates_to_dataframe(data = stops, decimal_places = 3, start_or_stop = \"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Remove original coordinates, and put those tuples in a dedicated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:34.998431234Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starts = starts.drop(columns = [\"start_latitude\", \"start_longitude\"])\n",
    "stops = stops.drop(columns = [\"stop_latitude\", \"stop_longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Add columns of consisting of tuples of rounded coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:34.998805263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_column_of_rounded_points(data = starts, start_or_stop = \"start\")\n",
    "add_column_of_rounded_points(data = stops, start_or_stop = \"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:34.999872822Z"
    }
   },
   "outputs": [],
   "source": [
    "starts = starts.drop(columns = [\"rounded_start_latitude\", \"rounded_start_longitude\"])\n",
    "stops = stops.drop(columns = [\"rounded_stop_latitude\", \"rounded_stop_longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Make new location IDs, and associate each of them to a point using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.000305670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1833it [00:00, 1355936.37it/s]\n",
      "1241it [00:00, 1689977.68it/s]\n"
     ]
    }
   ],
   "source": [
    "origins_and_ids = make_new_station_ids(data = starts, scenario=\"start\")\n",
    "destinations_and_ids = make_new_station_ids(data = stops, scenario=\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensuring that any points common to the origins and destinations have the same IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We have made dictionaries associating the origins and destinations with IDs, but some of the destinations and origins may be the same. And they will have been assigned to different IDs. These common locations (or rather, their coordinates) must be assigned to the same ID in each dictionary.\n",
    "\n",
    "\n",
    "###### First, let us find out how many of these points are common to these dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.000749862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_points = [point for point in destinations_and_ids.keys() if point in origins_and_ids.keys()]\n",
    "len(common_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There are 2,357 common locations. And they will most likely have been assigned to different IDs in each dictionary.\n",
    "###### Let us ensure that these common points have the same IDs in each dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.001127Z"
    }
   },
   "outputs": [],
   "source": [
    "for point in common_points:\n",
    "\n",
    "        destinations_and_ids[point] = origins_and_ids[point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking for repetition of origin IDs. The presence of repeated values necessitates a deeper investigation into whether they are shared by two different points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.001499389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(origins_and_ids.values()) == len(set(origins_and_ids.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There are no repeated origin IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.001899942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(destinations_and_ids.values()) == len(set(destinations_and_ids.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There are some repeated destination IDs. Let us check whether they belong to the same points or not. If they belong to two different points, then that will have to be rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.002384422Z"
    }
   },
   "outputs": [],
   "source": [
    "for a,b in zip(destinations_and_ids.keys(), destinations_and_ids.keys()):\n",
    "\n",
    "    if destinations_and_ids[a] == destinations_and_ids[b] and a != b:\n",
    "\n",
    "        print((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All clear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save these dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is crucial because it will allow me to recall this particular association of coordinates with IDs later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.003390285Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dict(\n",
    "    folder = GEOGRAPHICAL_DATA, \n",
    "    dictionary = origins_and_ids, \n",
    "    file_name = \"rounded_origin_points_and_their_IDs.pkl\"\n",
    "    )\n",
    "\n",
    "save_dict(\n",
    "    folder = GEOGRAPHICAL_DATA, \n",
    "    dictionary = destinations_and_ids, \n",
    "    file_name = \"rounded_destination_points_and_their_IDs.pkl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Form a column of said IDs (in the appropriate order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.049112317Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_column_of_ids(data = starts, start_or_stop = \"start\", points_and_ids = origins_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.049414444Z"
    }
   },
   "outputs": [],
   "source": [
    "add_column_of_ids(data = stops, start_or_stop = \"stop\", points_and_ids = destinations_and_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Form Aggregate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.049712756Z"
    }
   },
   "outputs": [],
   "source": [
    "starts = starts.drop(\"rounded_start_points\", axis = 1)\n",
    "stops = stops.drop(\"rounded_stop_points\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Aggregate Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.049930690Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_starts = starts.groupby([\"start_hour\", \"start_station_id\"]).size().reset_index()\n",
    "agg_starts = agg_starts.rename(columns = {0: \"trips\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Aggregate Starts with Missing Slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_starts = pd.read_parquet(path = TEMPORARY_DATA/\"agg_starts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.050123927Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_starts = add_missing_slots(agg_data = agg_starts, start_or_stop = \"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.050348109Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_starts.to_parquet(path = TRANSFORMED_DATA/\"ts_starts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.050558985Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_stops = stops.groupby([\"stop_hour\", \"stop_station_id\"]).size().reset_index()\n",
    "agg_stops = agg_stops.rename(columns = {0: \"trips\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Aggregate Stops with Missing Slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.050804715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1833/1833 [01:18<00:00, 23.49it/s]\n"
     ]
    }
   ],
   "source": [
    "ts_stops = add_missing_slots(agg_data = agg_stops, start_or_stop = \"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T14:13:35.051037452Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_stops.to_parquet(path = TRANSFORMED_DATA/\"ts_stops.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
